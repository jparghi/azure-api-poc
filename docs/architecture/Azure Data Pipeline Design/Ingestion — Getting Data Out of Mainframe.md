## üß© **1Ô∏è‚É£ Ingestion ‚Äî Getting Data Out of Mainframe**

| Step                             | Tool / Method                                              | What Happens                                                               | Notes                              |
| -------------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------------------- | ---------------------------------- |
| **a. Data Extraction (on-prem)** | IBM DFSORT / JCL Jobs / IBM Connect Direct / FTP (SFTP)    | Nightly COBOL/VSAM data exported as **flat files** or **JSON/XML**.        | Sometimes zipped or tarred.        |
| **b. Format Conversion**         | **IBM DataStage** or **Azure Data Factory Self-Hosted IR** | Converts **EBCDIC ‚Üí UTF-8** and **fixed-width ‚Üí Delimited (Parquet/CSV)**. | Use schema mapping from copybooks. |
| **c. Transfer to Cloud**         | **SFTP ‚Üí Blob Storage / ADLS Gen2 /raw**                   | Files uploaded securely via **SAS token** or **Private Endpoint**.         | Triggers downstream processing.    |

‚úÖ **Result:** Mainframe dumps land as **raw, UTF-8, delimited files** in `/raw/mainframe/YYYY/MM/DD`.

---

## ‚öôÔ∏è **2Ô∏è‚É£ Schema & Metadata Extraction**

### Option A ‚Äî COBOL Copybook Parser

* Use **ADF Mapping Data Flow** or **Databricks Copybook Library**:

  ```python
  from com.databricks.spark.avro import CobolParser
  df = CobolParser.read("policy.cpy").fromText("input.dat")
  df.write.format("parquet").save("/raw/mainframe_parsed/")
  ```
* Auto-converts fixed-width into columns using COBOL schema.
* Handles PIC clauses, COMP-3 fields, decimals, etc.

### Option B ‚Äî Flat File Definition Table

* Maintain a JSON schema describing each file layout.
* Azure Data Factory can map fields dynamically.

‚úÖ **Goal:** End up with *structured Parquet files* in `/bronze/mainframe/‚Ä¶`.

---

## üßÆ **3Ô∏è‚É£ Data Cleansing & Transformation**

| Type               | Technology                       | Example                                             |
| ------------------ | -------------------------------- | --------------------------------------------------- |
| **Basic rules**    | Azure Data Factory Data Flows    | Filter invalid records, cast data types.            |
| **Heavy ETL**      | Azure Databricks (Spark)         | Deduplicate, join reference data, enrich claims.    |
| **Business rules** | Delta Live Tables (Databricks)   | Apply claim eligibility logic, normalize code sets. |
| **Anonymization**  | Azure Functions / Databricks UDF | Mask SSN, DOB, and other PHI fields.                |

‚úÖ Bronze ‚Üí Silver ‚Üí Gold Delta tables created in ADLS Gen2.

---

## üîÑ **4Ô∏è‚É£ Automation & Orchestration**

* **Azure Data Factory Pipeline:**

    1. SFTP copy ‚Üí ADLS /raw
    2. Databricks notebook ‚Üí ETL job
    3. Validation + Notifications

* **Delta Live Tables:** Automates incremental processing, CDC, and audit logging.

* **Azure Synapse Pipeline:** Alternative if you prefer SQL-centric orchestration.

---

## üìä **5Ô∏è‚É£ Consumption & Analytics**

* **Synapse Serverless SQL** ‚Äî ad-hoc queries on `/gold/mainframe/*.parquet`
* **Power BI / API Layer** ‚Äî interactive dashboards on processed data
* **Cosmos DB / Search** ‚Äî indexed subset for low-latency access

---

## üß† **6Ô∏è‚É£ Governance & Compliance**

| Concern                      | Mechanism                                                   |
| ---------------------------- | ----------------------------------------------------------- |
| **Lineage & Cataloging**     | Microsoft Purview auto-scans ADLS and Delta tables.         |
| **Secrets / Keys**           | Azure Key Vault secures SFTP credentials, storage keys.     |
| **Audit & Monitoring**       | Azure Monitor + App Insights track pipeline latency & cost. |
| **HIPAA / PHIPA Compliance** | All PII masked; private endpoints enforced.                 |

---

## üìò **In Summary ‚Äî Mainframe Data Path**

```mermaid
flowchart LR
    MF["üèõÔ∏è Mainframe<br/>(VSAM ‚Ä¢ COBOL)"]
    SFTP["üîê Secure Transfer (SFTP / Connect Direct)"]
    RAW["üóÇÔ∏è ADLS Gen2 /raw<br/>EBCDIC ‚Üí UTF-8"]
    PARSE["üß© Databricks Copybook Parser<br/>or ADF Mapping Flow"]
    BRZ["ü•â Bronze Delta Table"]
    SIL["ü•à Silver Delta Table"]
    GLD["ü•á Gold Delta Table"]
    SYN["üß≠ Synapse SQL / Power BI"]

    MF --> SFTP --> RAW --> PARSE --> BRZ --> SIL --> GLD --> SYN
```

---

### üîë **Key Takeaways**

* ‚úÖ Extract using **IBM Connect Direct or ADF IR**  
* ‚úÖ Convert **EBCDIC ‚Üí UTF-8**, **fixed-width ‚Üí Parquet**  
* ‚úÖ Use **Databricks Copybook Parser** for schema alignment
* ‚úÖ Clean / mask / join ‚Üí Delta Lake (Bronze/Silver/Gold)
* ‚úÖ Query via **Synapse SQL** or **Power BI**

